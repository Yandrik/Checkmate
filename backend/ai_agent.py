import pprint
import urllib.parse
import json5
from qwen_agent.agents import Assistant #type: ignore
from qwen_agent.tools.base import BaseTool, register_tool #type: ignore
from qwen_agent.utils.output_beautify import typewriter_print #type: ignore


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool('fact_checker')
class MyFactChecker(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = 'A tool that checks the factuality of a given text. '
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        'name': 'prompt',
        'type': 'string',
        'description': 'What parts of the text are factual and what parts are not? ',
        'required': True
    }]

    def call(self, params: str, **kwargs) -> str: #type: ignore
        # `params` are the arguments generated by the LLM agent.
        data = json5.loads(params)
        if not isinstance(data, dict) or 'prompt' not in data:
            raise ValueError("Invalid params: expected a dict with a 'prompt' key")
        prompt = urllib.parse.quote(data['prompt'])
        return json5.dumps(
            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'}, #insert your frontend url here
            ensure_ascii=False)


# Step 2: Configure the LLM you are using.
llm_cfg = {
    # Use the model service provided by DashScope:
    'model': 'qwen-max-latest',
    'model_type': 'qwen_dashscope',
    # 'api_key': 'YOUR_DASHSCOPE_API_KEY',
    # It will use the `DASHSCOPE_API_KEY' environment variable if 'api_key' is not set here.

    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
    # 'model': 'Qwen2.5-7B-Instruct',
    # 'model_server': 'http://localhost:8000/v1',  # base_url, also known as api_base
    # 'api_key': 'EMPTY',

    # (Optional) LLM hyperparameters for generation:
    'generate_cfg': {
        'top_p': 0.8
    }
}

#
# ENTER PROMPT TO CREATE THE CORRECT BEHAVIOR OF THE AGENT HERE BELOW
#

# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
system_instruction = '''After receiving the user's request, you should:
- ignore all lines of text that are not relevant for the information of the text,
- slice the given text into sections with information about the topic,
- give each slice a number in ascending order
- search for information to the specific topics online (trusted sources)
- correct every wrong inforamtion and assess a score between 0 and 100 percent how wrong the inforamtion in the original text ist
- give out the corrected version and the score as an accuracy value`.'''
tools: list[str | dict | BaseTool] = ['fact_checker', 'code_interpreter']  # `code_interpreter` is a built-in tool for executing code.
files = ['aufgabenstellung.pdf']  # Give the bot a PDF file to read.
bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools,
                files=files)

# Step 4: Run the agent as a chatbot.
messages = []  # This stores the chat history.
while True:
    # For example, enter the query "draw a dog and rotate it 90 degrees".
    query = input('\nuser query: ')
    # Append the user query to the chat history.
    messages.append({'role': 'user', 'content': query})
    response = []
    response_plain_text = ''
    print('bot response:')
    for response in bot.run(messages=messages):
        # Streaming output.
        response_plain_text = typewriter_print(response, response_plain_text) #type: ignore
    # Append the bot responses to the chat history.
    messages.extend(response)